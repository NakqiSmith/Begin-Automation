{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard Creation and Automated Reporting Instruction Manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Application Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab and Open Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two cells deep do exactly what i want it do do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Importation\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Loading Necessary Workbooks\n",
    "wb1 = load_workbook (\"StaffPerformanceOverviewCR06-23-23 test copy.xlsx\")\n",
    "wb2 = load_workbook(\"StaffPerformanceOverviewNH06-23-23 test copy.xlsx\")\n",
    "ws1 = wb1.active\n",
    "ws2 = wb2.active\n",
    "\n",
    "#Appending Data To One Workbook\n",
    "for row in ws2.iter_rows(min_row=2, values_only=True): \n",
    "    ws1.append(row)\n",
    "\n",
    "wb1.save(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n",
    "\n",
    "#reload new workbook that was just saved\n",
    "wb = load_workbook(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n",
    "ws = wb.active\n",
    "\n",
    "#Remove Row to consolidate data\n",
    "start_row = 12\n",
    "num_rows_to_remove = 1\n",
    "ws.delete_rows(start_row, num_rows_to_remove)\n",
    "\n",
    "# Define the column letter you want to update (e.g., column A)\n",
    "column_letter = 'E'\n",
    "\n",
    "# Define the value to be replaced and its corresponding replacement\n",
    "old = \"NA\"\n",
    "new = \"0\"\n",
    "\n",
    "# Get the column index from the column letter\n",
    "column_index = ord(column_letter) - ord('A') + 1\n",
    "\n",
    "# Loop through the cells in the selected column and replace the old value with the new value\n",
    "for row in ws.iter_rows(min_row=3, min_col=column_index, max_col=column_index):\n",
    "    cell = row[0]\n",
    "    if cell.value == old:\n",
    "        cell.value = new\n",
    "\n",
    "from openpyxl.styles import NamedStyle\n",
    "\n",
    "# Create named style for the whole number format\n",
    "whole_number_style = NamedStyle(name=\"whole_number\", number_format=\"0\")\n",
    "\n",
    "# Columns that need to be formatted as a whole number (example: columns A, B, and D)\n",
    "columns_to_format = ['B', 'C', 'D']  # Replace with the letters of the columns you want to format\n",
    "\n",
    "# Apply the named style to each cell in the selected columns\n",
    "for column_letter in columns_to_format:\n",
    "    for cell in ws[column_letter]:\n",
    "        cell.style = whole_number_style\n",
    "\n",
    "Decimal_Number_Style = NamedStyle(name=\"decimal\", number_format=\"#,##0.00\")\n",
    "\n",
    "columns_to_format = ['E', 'F', 'G', 'H','I','J','K','L','M','N','O']\n",
    "\n",
    "for column_letter in columns_to_format:\n",
    "    for cell in ws[column_letter]:\n",
    "        cell.style = Decimal_Number_Style\n",
    "\n",
    "\n",
    "wb.save(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load the existing Excel workbook\n",
    "wb = load_workbook(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n",
    "\n",
    "# Select the worksheet you want to update (assuming it's the first sheet)\n",
    "ws = wb.active\n",
    "\n",
    "# Find the last row in the worksheet\n",
    "last_row = ws.max_row\n",
    "\n",
    "# Create a dictionary to store the rows of employees with multiple entries\n",
    "multiple_entries_rows = {}\n",
    "\n",
    "# Iterate through the rows and collect the rows of employees with multiple entries\n",
    "for row in range(3, last_row + 1):\n",
    "    employee_id = ws.cell(row=row, column=1).value  # Assuming 'EmployeeID' is in column A\n",
    "    if employee_id in multiple_entries_rows:\n",
    "        multiple_entries_rows[employee_id].append([cell.value for cell in ws[row]])\n",
    "    else:\n",
    "        multiple_entries_rows[employee_id] = [[cell.value for cell in ws[row]]]\n",
    "\n",
    "# Clear the existing data in the worksheet\n",
    "ws.delete_rows(3, last_row)\n",
    "\n",
    "# Write the rows of employees with single entries at the top of the worksheet\n",
    "starting_row_for_multiple_entries = 11  # Adjust this variable for different reports\n",
    "for employee_id, rows in multiple_entries_rows.items():\n",
    "    if len(rows) == 1:\n",
    "        ws.append(rows[0])  # Append the single row for employees with single entries\n",
    "\n",
    "# Write the rows of employees with multiple entries at the bottom of the worksheet starting at the specified row\n",
    "for employee_id, rows in multiple_entries_rows.items():\n",
    "    if len(rows) > 1:\n",
    "        for row_data in rows:\n",
    "            ws.insert_rows(starting_row_for_multiple_entries, 1)  # Insert a row at the specified row for employees with multiple entries\n",
    "            for col_index, cell_value in enumerate(row_data):\n",
    "                ws.cell(row=starting_row_for_multiple_entries, column=col_index + 1, value=cell_value)\n",
    "            starting_row_for_multiple_entries += 1  # Increment the starting row for the next employee\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load the existing Excel workbook\n",
    "wb = load_workbook(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n",
    "\n",
    "# Select the worksheet you want to update (assuming it's the first sheet)\n",
    "ws = wb.active\n",
    "\n",
    "# Read the data from the worksheet into a Pandas DataFrame\n",
    "data_range = ws['A11:O18']  # Assuming data is in columns A to K starting from row 2\n",
    "data = [[cell.value for cell in row] for row in data_range]\n",
    "\n",
    "# Create a DataFrame with all the data\n",
    "df = pd.DataFrame(data, columns=['EmployeeID', 'Data1', 'Data2', 'Data3', 'Data4', 'Data5', 'Data6', 'Data7', \n",
    "                                  'Data8', 'Data9', 'Data10', 'Data11', 'Data12', 'Data13', 'Data14'])\n",
    "\n",
    "# Get the union of EmployeeIDs\n",
    "all_employee_ids = set(df['EmployeeID'])\n",
    "\n",
    "# Create a DataFrame with all EmployeeIDs\n",
    "df_all_employees = pd.DataFrame({'EmployeeID': list(all_employee_ids)})\n",
    "\n",
    "# Merge the original data with the DataFrame containing all EmployeeIDs (outer merge)\n",
    "merged_data = pd.merge(df_all_employees, df, on='EmployeeID', how='left')\n",
    "\n",
    "# Fill NaN values with 0\n",
    "merged_data.fillna(0, inplace=True)\n",
    "\n",
    "# Convert 'Data4', 'Data13', and 'Data14' columns to numeric type (ignore errors)\n",
    "merged_data['Data4'] = pd.to_numeric(merged_data['Data4'], errors='coerce')\n",
    "merged_data['Data13'] = pd.to_numeric(merged_data['Data13'], errors='coerce')\n",
    "merged_data['Data14'] = pd.to_numeric(merged_data['Data14'], errors='coerce')\n",
    "\n",
    "# Consolidate data based on EmployeeID and calculate the sum for each of the 14 columns\n",
    "consolidated_data = merged_data.groupby('EmployeeID', as_index=False).agg({\n",
    "    'Data1': 'sum',\n",
    "    'Data2': 'sum',\n",
    "    'Data3': 'sum',\n",
    "    'Data4': 'sum',\n",
    "    'Data5': 'sum',\n",
    "    'Data6': 'sum',\n",
    "    'Data7': 'sum',\n",
    "    'Data8': 'sum',\n",
    "    'Data9': 'sum',\n",
    "    'Data10': 'sum',\n",
    "    'Data11': 'sum',\n",
    "    'Data12': 'sum',\n",
    "    'Data13': 'sum',\n",
    "    'Data14': 'sum'})\n",
    "\n",
    "# Calculate the average for Data4, Data13, and Data14 columns for each individual employee\n",
    "consolidated_data['Data4'] = consolidated_data['Data4'] / 2\n",
    "consolidated_data['Data13'] = consolidated_data['Data13'] / 2\n",
    "consolidated_data['Data14'] = consolidated_data['Data14'] / 2\n",
    "\n",
    "# Determine the starting row for placing the consolidated data\n",
    "start_row = 20  # Assuming you want to start placing the consolidated data in row 20\n",
    "\n",
    "# Put the consolidated data back into the Excel worksheet\n",
    "for row_index, row_data in enumerate(consolidated_data.values):\n",
    "    for col_index, cell_value in enumerate(row_data):\n",
    "        cell = ws.cell(row=start_row + row_index, column=col_index + 1)\n",
    "        cell.value = cell_value\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load the existing Excel workbook\n",
    "wb = load_workbook(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n",
    "\n",
    "# Select the worksheet you want to update (assuming it's the first sheet)\n",
    "ws = wb.active\n",
    "\n",
    "# Read the data from the worksheet into a Pandas DataFrame\n",
    "data_range = ws['A3:O18']  # Assuming data is in columns A to K starting from row 2\n",
    "data = [[cell.value for cell in row] for row in data_range]\n",
    "\n",
    "# Create a DataFrame with all the data\n",
    "df = pd.DataFrame(data, columns=['EmployeeID', 'Data1', 'Data2', 'Data3', 'Data4', 'Data5', 'Data6', 'Data7', \n",
    "                                  'Data8', 'Data9', 'Data10', 'Data11', 'Data12', 'Data13', 'Data14'])\n",
    "\n",
    "# Convert 'Data4', 'Data13', and 'Data14' to numeric (ignore errors)\n",
    "df['Data4'] = pd.to_numeric(df['Data4'], errors='coerce')\n",
    "df['Data13'] = pd.to_numeric(df['Data13'], errors='coerce')\n",
    "df['Data14'] = pd.to_numeric(df['Data14'], errors='coerce')\n",
    "\n",
    "# Get the union of EmployeeIDs\n",
    "all_employee_ids = set(df['EmployeeID'])\n",
    "\n",
    "# Create a DataFrame with all EmployeeIDs\n",
    "df_all_employees = pd.DataFrame({'EmployeeID': list(all_employee_ids)})\n",
    "\n",
    "# Merge the original data with the DataFrame containing all EmployeeIDs (outer merge)\n",
    "merged_data = pd.merge(df_all_employees, df, on='EmployeeID', how='left')\n",
    "\n",
    "# Fill NaN values with 0\n",
    "merged_data.fillna(0, inplace=True)\n",
    "\n",
    "# Consolidate data based on EmployeeID and calculate the sum for each of the 8 columns\n",
    "sum_columns = ['Data1', 'Data2', 'Data3', 'Data5', 'Data6', 'Data7', 'Data8', 'Data9', 'Data10', 'Data11', 'Data12']\n",
    "consolidated_data = merged_data.groupby('EmployeeID', as_index=False)[sum_columns].sum()\n",
    "\n",
    "# Calculate the average for Data4, Data13, and Data14 columns and divide by 2\n",
    "consolidated_data['Data4'] = merged_data['Data4'].groupby(merged_data['EmployeeID']).transform('mean') / 2\n",
    "consolidated_data['Data13'] = merged_data['Data13'].groupby(merged_data['EmployeeID']).transform('mean') / 2\n",
    "consolidated_data['Data14'] = merged_data['Data14'].groupby(merged_data['EmployeeID']).transform('mean') / 2\n",
    "\n",
    "# Determine the starting row for placing the consolidated data\n",
    "start_row = 20  # Assuming you want to start placing the consolidated data in row 20\n",
    "\n",
    "# Put the consolidated data back into the Excel worksheet\n",
    "for row_index, row_data in enumerate(consolidated_data.values):\n",
    "    for col_index, cell_value in enumerate(row_data):\n",
    "        cell = ws.cell(row=start_row + row_index, column=col_index + 1)\n",
    "        cell.value = cell_value\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below does exactly what i want right after the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Data12'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Data12'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m consolidated_data_both_locations \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(consolidated_data_location1, consolidated_data_location2, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEmployeeID\u001b[39m\u001b[39m'\u001b[39m, suffixes\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m_loc1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_loc2\u001b[39m\u001b[39m'\u001b[39m), how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[39m# Convert 'Data12', 'Data4', 'Data13', and 'Data14' columns to numeric types\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m consolidated_data_both_locations[\u001b[39m'\u001b[39m\u001b[39mData12\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_numeric(consolidated_data_both_locations[\u001b[39m'\u001b[39;49m\u001b[39mData12\u001b[39;49m\u001b[39m'\u001b[39;49m], errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoerce\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m consolidated_data_both_locations[\u001b[39m'\u001b[39m\u001b[39mData4\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_numeric(consolidated_data_both_locations[\u001b[39m'\u001b[39m\u001b[39mData4\u001b[39m\u001b[39m'\u001b[39m], errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoerce\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m consolidated_data_both_locations[\u001b[39m'\u001b[39m\u001b[39mData13\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_numeric(consolidated_data_both_locations[\u001b[39m'\u001b[39m\u001b[39mData13\u001b[39m\u001b[39m'\u001b[39m], errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoerce\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Data12'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load the existing Excel workbook\n",
    "wb = load_workbook(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n",
    "\n",
    "# Select the worksheet you want to update (assuming it's the first sheet)\n",
    "ws = wb.active\n",
    "\n",
    "# Read the data from the worksheet into a Pandas DataFrame\n",
    "data_range_location1 = ws['A3:O11']  # Assuming data for location 1 is in columns A to K starting from row 2\n",
    "data_location1 = [[cell.value for cell in row] for row in data_range_location1]\n",
    "\n",
    "data_range_location2 = ws['A12:O18']  # Assuming data for location 2 is in columns A to K starting from row 2\n",
    "data_location2 = [[cell.value for cell in row] for row in data_range_location2]\n",
    "\n",
    "# Create separate DataFrames for location 1 and location 2\n",
    "df_location1 = pd.DataFrame(data_location1, columns=['EmployeeID', 'Data1', 'Data2', 'Data3', 'Data4', 'Data5', 'Data6', 'Data7', \n",
    "                                                                                            'Data8', 'Data9', 'Data10', 'Data11', 'Data12', 'Data13', 'Data14'])\n",
    "df_location2 = pd.DataFrame(data_location2, columns=['EmployeeID', 'Data1', 'Data2', 'Data3', 'Data4', 'Data5', 'Data6', 'Data7',\n",
    "                                                                                             'Data8', 'Data9', 'Data10', 'Data11', 'Data12', 'Data13', 'Data14'])\n",
    "\n",
    "# Get the union of EmployeeIDs from both data frames\n",
    "all_employee_ids = set(df_location1['EmployeeID']).union(set(df_location2['EmployeeID']))\n",
    "\n",
    "# Create DataFrames with all EmployeeIDs for both locations\n",
    "df_all_location1 = pd.DataFrame({'EmployeeID': list(all_employee_ids)})\n",
    "df_all_location2 = pd.DataFrame({'EmployeeID': list(all_employee_ids)})\n",
    "\n",
    "# Merge the original data with the DataFrames containing all EmployeeIDs (outer merge)\n",
    "merged_location1 = pd.merge(df_all_location1, df_location1, on='EmployeeID', how='left')\n",
    "merged_location2 = pd.merge(df_all_location2, df_location2, on='EmployeeID', how='left')\n",
    "\n",
    "# Fill NaN values with 0\n",
    "merged_location1.fillna(0, inplace=True)\n",
    "merged_location2.fillna(0, inplace=True)\n",
    "\n",
    "# Consolidate data based on EmployeeID and calculate the sum for each location\n",
    "consolidated_data_location1 = merged_location1.groupby('EmployeeID', as_index=False).sum()\n",
    "consolidated_data_location2 = merged_location2.groupby('EmployeeID', as_index=False).sum()\n",
    "\n",
    "# Merge the consolidated data for employees working at both locations (outer merge)\n",
    "consolidated_data_both_locations = pd.merge(consolidated_data_location1, consolidated_data_location2, on='EmployeeID', suffixes=('_loc1', '_loc2'), how='outer')\n",
    "\n",
    "# Calculate the final values for the Data1, Data2, Data3 columns (sum for all)\n",
    "consolidated_data_both_locations['Data1'] = consolidated_data_both_locations['Data1_loc1'] + consolidated_data_both_locations['Data1_loc2']\n",
    "consolidated_data_both_locations['Data2'] = consolidated_data_both_locations['Data2_loc1'] + consolidated_data_both_locations['Data2_loc2']\n",
    "consolidated_data_both_locations['Data3'] = consolidated_data_both_locations['Data3_loc1'] + consolidated_data_both_locations['Data3_loc2']\n",
    "consolidated_data_both_locations['Data5'] = consolidated_data_both_locations['Data5_loc1'] + consolidated_data_both_locations['Data5_loc2']\n",
    "consolidated_data_both_locations['Data6'] = consolidated_data_both_locations['Data6_loc1'] + consolidated_data_both_locations['Data6_loc2']\n",
    "consolidated_data_both_locations['Data7'] = consolidated_data_both_locations['Data7_loc1'] + consolidated_data_both_locations['Data7_loc2']\n",
    "consolidated_data_both_locations['Data8'] = consolidated_data_both_locations['Data8_loc1'] + consolidated_data_both_locations['Data8_loc2']\n",
    "consolidated_data_both_locations['Data9'] = consolidated_data_both_locations['Data9_loc1'] + consolidated_data_both_locations['Data9_loc2']\n",
    "consolidated_data_both_locations['Data10'] = consolidated_data_both_locations['Data10_loc1'] + consolidated_data_both_locations['Data10_loc2']\n",
    "consolidated_data_both_locations['Data11'] = consolidated_data_both_locations['Data11_loc1'] + consolidated_data_both_locations['Data11_loc2']\n",
    "consolidated_data_both_locations['Data12'] = consolidated_data_both_locations['Data12_loc1'] + consolidated_data_both_locations['Data12_loc2']\n",
    "\n",
    "# Calculate the average for Data4 to Data6 columns\n",
    "consolidated_data_both_locations['Data4'] = (consolidated_data_both_locations['Data4_loc1'] + consolidated_data_both_locations['Data4_loc2']) / 2\n",
    "consolidated_data_both_locations['Data13'] = (consolidated_data_both_locations['Data13_loc1'] + consolidated_data_both_locations['Data13_loc2']) / 2\n",
    "consolidated_data_both_locations['Data14'] = (consolidated_data_both_locations['Data14_loc1'] + consolidated_data_both_locations['Data14_loc2']) / 2\n",
    "\n",
    "# Remove the unnecessary columns from the consolidated DataFrame\n",
    "consolidated_data_both_locations.drop(columns=[\n",
    "    'Data1_loc1', 'Data1_loc2', 'Data2_loc1', 'Data2_loc2', 'Data3_loc1', 'Data3_loc2',\n",
    "    'Data4_loc1', 'Data4_loc2', 'Data5_loc1', 'Data5_loc2', 'Data6_loc1', 'Data6_loc2',\n",
    "    'Data7_loc1', 'Data7_loc2', 'Data8_loc1', 'Data8_loc2', 'Data9_loc1', 'Data9_loc2', \n",
    "    'Data10_loc1', 'Data10_loc2', 'Data11_loc1', 'Data11_loc2', 'Data12_loc1', 'Data12_loc2',\n",
    "     'Data13_loc1', 'Data13_loc2',  'Data14_loc1', 'Data14_loc2'], inplace=True)\n",
    "\n",
    "# Convert the consolidated data to a list\n",
    "consolidated_data_list = consolidated_data_both_locations.values.tolist()\n",
    "\n",
    "# Determine the starting row for placing the consolidated data\n",
    "start_row = 20  # Assuming you want to start placing the consolidated data in row 20\n",
    "\n",
    "# Put the consolidated data back into the Excel worksheet\n",
    "for row_index, row_data in enumerate(consolidated_data_list):\n",
    "    for col_index, cell_value in enumerate(row_data):\n",
    "        cell = ws.cell(row=start_row + row_index, column=col_index + 1)\n",
    "        cell.value = cell_value\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load the existing Excel workbook\n",
    "wb = load_workbook(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n",
    "\n",
    "# Select the worksheet you want to update (assuming it's the first sheet)\n",
    "ws = wb.active\n",
    "\n",
    "# Read the data from the worksheet into a Pandas DataFrame\n",
    "data_range_location1 = ws['A3:D11']  # Assuming data for location 1 is in column A starting from row 2\n",
    "data_location1 = [[cell.value for cell in row] for row in data_range_location1]\n",
    "\n",
    "data_range_location2 = ws['A12:D18']  # Assuming data for location 2 is in column E starting from row 2\n",
    "data_location2 = [[cell.value for cell in row] for row in data_range_location2]\n",
    "\n",
    "# Create separate DataFrames for location 1 and location 2\n",
    "df_location1 = pd.DataFrame(data_location1, columns=['EmployeeID', 'Data1', 'Data2', 'Data3'])\n",
    "df_location2 = pd.DataFrame(data_location2, columns=['EmployeeID', 'Data1', 'Data2', 'Data3'])\n",
    "\n",
    "# Consolidate data based on EmployeeID and sum the values for each location\n",
    "consolidated_data_location1 = df_location1.groupby('EmployeeID', as_index=False).sum()\n",
    "consolidated_data_location2 = df_location2.groupby('EmployeeID', as_index=False).sum()\n",
    "\n",
    "# Merge the consolidated data for employees working at both locations (outer merge)\n",
    "consolidated_data_both_locations = pd.merge(consolidated_data_location1, consolidated_data_location2, on='EmployeeID', suffixes=('_loc1', '_loc2'), how='inner')\n",
    "\n",
    "# Calculate the final values for the Data1, Data2, Data3 columns (sum for all)\n",
    "consolidated_data_both_locations['Data1'] = consolidated_data_both_locations['Data1_loc1'] + consolidated_data_both_locations['Data1_loc2']\n",
    "consolidated_data_both_locations['Data2'] = consolidated_data_both_locations['Data2_loc1'] + consolidated_data_both_locations['Data2_loc2']\n",
    "consolidated_data_both_locations['Data3'] = consolidated_data_both_locations['Data3_loc1'] + consolidated_data_both_locations['Data3_loc2']\n",
    "\n",
    "# Remove the unnecessary columns from the consolidated DataFrame\n",
    "consolidated_data_both_locations.drop(columns=['Data1_loc1', 'Data1_loc2', 'Data2_loc1', 'Data2_loc2', 'Data3_loc1', 'Data3_loc2'], inplace=True)\n",
    "\n",
    "# Convert the consolidated data to a list\n",
    "consolidated_data_list = consolidated_data_both_locations.values.tolist()\n",
    "\n",
    "# Determine the starting row for placing the consolidated data\n",
    "start_row = 20  # Assuming you want to start placing the consolidated data in row 20\n",
    "\n",
    "# Put the consolidated data back into the Excel worksheet\n",
    "for row_index, row_data in enumerate(consolidated_data_list):\n",
    "    for col_index, cell_value in enumerate(row_data):\n",
    "        cell = ws.cell(row=start_row + row_index, column=col_index + 1)\n",
    "        cell.value = cell_value\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load the existing Excel workbook\n",
    "wb = load_workbook(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n",
    "\n",
    "# Select the worksheet you want to update (assuming it's the first sheet)\n",
    "ws = wb.active\n",
    "\n",
    "# Read the data from the worksheet into a Pandas DataFrame\n",
    "data_range_location1 = ws['A3:D10']  # Assuming data for location 1 is in column A starting from row 2\n",
    "data_location1 = [[cell.value for cell in row] for row in data_range_location1]\n",
    "\n",
    "data_range_location2 = ws['A11:D18']  # Assuming data for location 2 is in column E starting from row 2\n",
    "data_location2 = [[cell.value for cell in row] for row in data_range_location2]\n",
    "\n",
    "# Create separate DataFrames for location 1 and location 2\n",
    "df_location1 = pd.DataFrame(data_location1, columns=['EmployeeID', 'Data1', 'Data2', 'Data3'])\n",
    "df_location2 = pd.DataFrame(data_location2, columns=['EmployeeID', 'Data1', 'Data2', 'Data3'])\n",
    "\n",
    "# Consolidate data based on EmployeeID and sum the values for each location\n",
    "consolidated_data_location1 = df_location1.groupby('EmployeeID', as_index=False).sum()\n",
    "consolidated_data_location2 = df_location2.groupby('EmployeeID', as_index=False).sum()\n",
    "\n",
    "# Merge the consolidated data for employees working at both locations\n",
    "consolidated_data_both_locations = pd.merge(consolidated_data_location1, consolidated_data_location2, on='EmployeeID', suffixes=('_loc1', '_loc2'), how='inner')\n",
    "consolidated_data_both_locations['Data1'] = consolidated_data_both_locations['Data1_loc1'] + consolidated_data_both_locations['Data1_loc2']\n",
    "consolidated_data_both_locations['Data2'] = consolidated_data_both_locations['Data2_loc1'] + consolidated_data_both_locations['Data2_loc2']\n",
    "consolidated_data_both_locations['Data3'] = consolidated_data_both_locations['Data3_loc1'] + consolidated_data_both_locations['Data3_loc2']\n",
    "\n",
    "# Remove the unnecessary columns from the consolidated DataFrame\n",
    "consolidated_data_both_locations.drop(columns=['Data1_loc1', 'Data1_loc2', 'Data2_loc1', 'Data2_loc2', 'Data3_loc1', 'Data3_loc2'], inplace=True)\n",
    "\n",
    "# Convert the consolidated data to a list\n",
    "consolidated_data_list = consolidated_data_both_locations.values.tolist()\n",
    "\n",
    "# Determine the starting row for placing the consolidated data\n",
    "start_row = 3  # Assuming you want to start placing the consolidated data in row 2\n",
    "\n",
    "# Put the consolidated data back into the Excel worksheet\n",
    "for row_index, row_data in enumerate(consolidated_data_list):\n",
    "    for col_index, cell_value in enumerate(row_data):\n",
    "        cell = ws.cell(row=start_row + row_index, column=col_index + 1)\n",
    "        cell.value = cell_value\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(\"StaffPerformanceOverviewtest-06-23 test copy.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reportwflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
